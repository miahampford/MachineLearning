{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS 4501 Assignment 4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "-40VPC7MAGGB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Assignment 4: Benchmarking Fashion-MNIST with Deep Neural Nets"
      ]
    },
    {
      "metadata": {
        "id": "piFzh10hAGGE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### CS 4501 Machine Learning - Department of Computer Science - University of Virginia\n",
        "\"The original MNIST dataset contains a lot of handwritten digits. Members of the AI/ML/Data Science community love this dataset and use it as a benchmark to validate their algorithms. In fact, MNIST is often the first dataset researchers try. \"If it doesn't work on MNIST, it won't work at all\", they said. \"Well, if it does work on MNIST, it may still fail on others.\" - **Zalando Research, Github Repo.**\"\n",
        "\n",
        "Fashion-MNIST is a dataset from the Zalando's article. Each example is a 28x28 grayscale image, associated with a label from 10 classes. They intend Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms.\n",
        "\n",
        "![Here's an example how the data looks (each class takes three-rows):](https://github.com/zalandoresearch/fashion-mnist/raw/master/doc/img/fashion-mnist-sprite.png)\n",
        "\n",
        "In this assignment, you will attempt to benchmark the Fashion-MNIST using Neural Networks. You must use it to train some neural networks on TensorFlow and predict the final output of 10 classes. For deliverables, you must write code in Python and submit this Jupyter Notebook file (.ipynb) to earn a total of 100 pts. You will gain points depending on how you perform in the following sections.\n"
      ]
    },
    {
      "metadata": {
        "id": "469YvvIzAGGJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# You might want to use the following packages\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "tf.logging.set_verbosity(tf.logging.ERROR) #reduce annoying warning messages\n",
        "from functools import partial\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "def reset_graph(seed=42):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t-PtpH4xAGGG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 1. PRE-PROCESSING THE DATA (10 pts)\n",
        "\n",
        "You can load the Fashion MNIST directly from Tensorflow. **Partition of the dataset** so that you will have 50,000 examples for training, 10,000 examples for validation, and 10,000 examples for testing. Also, make sure that you platten out each of examples so that it contains only a 1-D feature vector.\n",
        "\n",
        "Write some code to output the dimensionalities of each partition (train, validation, and test sets).\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "L1NnJHz6JStN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "295c6547-084d-4b57-ace9-c473e224d233"
      },
      "cell_type": "code",
      "source": [
        "#split into 50000 training, 10000 validation, 10000 testing\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data();\n",
        "x_valid, x_train = x_train[:10000], x_train[10000:]\n",
        "y_valid, y_train = y_train[:10000], y_train[10000:]\n",
        "\n",
        "print(\"Number of training set examples: \", y_train.size)\n",
        "print(\"Number of validation set examples: \", y_valid.size)\n",
        "print(\"Number of testing set examples: \", y_test.size)\n",
        "print(\"\\n\")\n",
        "print(\"Dimensions training (before flattening): \", x_train.shape)\n",
        "print(\"Dimensions testing (before flattening): \", x_test.shape)\n",
        "print(\"Dimensions validation (before flattening): \", x_valid.shape)\n",
        "print(\"\\n\")\n",
        "\n",
        "x_train = x_train.astype(np.float32).reshape(-1, 784) / 255.0\n",
        "x_test = x_test.astype(np.float32).reshape(-1, 784) / 255.0\n",
        "x_valid = x_valid.astype(np.float32).reshape(-1, 784) / 255.0\n",
        "\n",
        "y_train = y_train.astype(np.int32)\n",
        "y_test = y_test.astype(np.int32)\n",
        "y_valid = y_valid.astype(np.int32)\n",
        "\n",
        "print(\"Dimensions training: \", x_train.shape)\n",
        "print(\"Dimensions testing: \", x_test.shape)\n",
        "print(\"Dimensions validation: \", x_valid.shape)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training set examples:  50000\n",
            "Number of validation set examples:  10000\n",
            "Number of testing set examples:  10000\n",
            "\n",
            "\n",
            "Dimensions training (before flattening):  (50000, 28, 28)\n",
            "Dimensions testing (before flattening):  (10000, 28, 28)\n",
            "Dimensions validation (before flattening):  (10000, 28, 28)\n",
            "\n",
            "\n",
            "Dimensions training:  (50000, 784)\n",
            "Dimensions testing:  (10000, 784)\n",
            "Dimensions validation:  (10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Na4CpxLBAGGP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- - -\n",
        "## 2. CONSTRUCTION PHASE (30 pts)\n",
        "\n",
        "In this section, define at least three neural networks with different structures. Make sure that the input layer has the right number of inputs. The best structure often is found through a process of trial and error experimentation:\n",
        "- You may start with a fully connected network structure with two hidden layers.\n",
        "- You may try a few settings of the number of nodes in each layer.\n",
        "- You may try a few activation functions to see if they affect the performance.\n",
        "\n",
        "**Important Implementation Note:** For the purpose of learning Tensorflow, you must use low-level TensorFlow API to construct the network. Usage of high-level tools (ie. Keras) is not permited. "
      ]
    },
    {
      "metadata": {
        "id": "bIJrHPVlAGGQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Your code goes here\n",
        "reset_graph()\n",
        "\n",
        "# Set some configuration here\n",
        "n_inputs = 28*28  # Fashion-MNIST\n",
        "learning_rate = 0.01\n",
        "n_outputs = 10\n",
        "\n",
        "# Construct placeholder for the input layer\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iDrFp7KKils6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_h1 = 300\n",
        "n_h2 = 100\n",
        "\n",
        "#implementation of the first net\n",
        "with tf.name_scope(\"dnn1\"):\n",
        "  h1 = tf.layers.dense(X, n_h1, name=\"hidden1\", activation=tf.nn.relu)\n",
        "  h2 = tf.layers.dense(X, n_h2, name=\"hidden2\", activation=tf.nn.relu)\n",
        "  logits = tf.layers.dense(h2, n_outputs, name = \"outputs\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "anUlPJtPiblR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#implementation of the loss function net\n",
        "with tf.name_scope(\"loss1\"):\n",
        "  xentropy1 = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "  loss1 = tf.reduce_mean(xentropy1, name=\"loss1\")\n",
        "  loss_summary1 = tf.summary.scalar('log_loss1', loss1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mVmXy956irRM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn_rate = .01\n",
        "\n",
        "#implementation of the training optimizer\n",
        "with tf.name_scope(\"train1\"):\n",
        "  optimizer1 = tf.train.GradientDescentOptimizer(learn_rate)\n",
        "  train_op1 = optimizer1.minimize(loss1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2s8bTClRiti9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#implementation of the evaluation procedure\n",
        "with tf.name_scope(\"eval1\"):\n",
        "  correct1 = tf.nn.in_top_k(logits,y,1)\n",
        "  accuracy1 = tf.reduce_mean(tf.cast(correct1, tf.float32))\n",
        "  accuracy_summary1 = tf.summary.scalar('accuracy1', accuracy1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "luyNllyVkNxr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_hid1 = 300\n",
        "n_hid2 = 200\n",
        "n_hid3 = 100\n",
        "n_hid4 = 75\n",
        "n_hid5 = 50\n",
        "\n",
        "#implementation of the second net\n",
        "with tf.name_scope(\"dnn2\"):\n",
        "  hid1 = tf.layers.dense(X, n_hid1, name=\"hidden1_2\", activation=tf.nn.relu)\n",
        "  hid2 = tf.layers.dense(X, n_hid2, name=\"hidden2_2\", activation=tf.nn.relu)\n",
        "  hid3 = tf.layers.dense(X, n_hid3, name=\"hidden3_2\", activation=tf.nn.relu)\n",
        "  hid4 = tf.layers.dense(X, n_hid4, name=\"hidden4_2\", activation=tf.nn.relu)      \n",
        "  hid5 = tf.layers.dense(X, n_hid5, name=\"hidden5_2\", activation=tf.nn.relu)\n",
        "  logits = tf.layers.dense(hid5, n_outputs, name = \"outputs_2\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7C97H2MHi8Le",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#implementation of the loss function net \n",
        "with tf.name_scope(\"loss2\"):\n",
        "  xentropy2 = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "  loss2 = tf.reduce_mean(xentropy2, name=\"loss2\")\n",
        "  loss_summary2 = tf.summary.scalar('log_loss2', loss2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IuMaVY9hjIZj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn_rate = .01\n",
        "\n",
        "#implementation of the training optimizer\n",
        "with tf.name_scope(\"train2\"):\n",
        "  optimizer2 = tf.train.GradientDescentOptimizer(learn_rate)\n",
        "  train_op2 = optimizer2.minimize(loss2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UeRD4yxsjIhp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#implementation of the evaluation procedure \n",
        "with tf.name_scope(\"eval2\"):\n",
        "  correct2 = tf.nn.in_top_k(logits,y,1)\n",
        "  accuracy2 = tf.reduce_mean(tf.cast(correct2, tf.float32))\n",
        "  accuracy_summary2 = tf.summary.scalar('accuracy2', accuracy2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JuvjZeJekP4-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_hid1 = 300\n",
        "n_hid2 = 150\n",
        "n_hid3 = 100\n",
        "\n",
        "#implementation of the third net \n",
        "with tf.name_scope(\"dnn3\"):\n",
        "  hi1 = tf.layers.dense(X, n_hid1, name=\"hidden1_3\", activation=tf.nn.tanh)\n",
        "  hi2 = tf.layers.dense(X, n_hid2, name=\"hidden2_3\", activation=tf.nn.relu)\n",
        "  hi3 = tf.layers.dense(X, n_hid3, name=\"hidden3_3\", activation=tf.nn.tanh)\n",
        "  logits = tf.layers.dense(hi3, n_outputs, name = \"outputs_3\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_mQCboA8ijWK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#implementation of the loss function net \n",
        "with tf.name_scope(\"loss3\"):\n",
        "  xentropy3 = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "  loss3 = tf.reduce_mean(xentropy3, name=\"loss3\")\n",
        "  loss_summary3 = tf.summary.scalar('log_loss3', loss3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GL_cXX09ih12",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn_rate = .01\n",
        "\n",
        "#implementation of the training optimizer \n",
        "with tf.name_scope(\"train3\"):\n",
        "  optimizer3 = tf.train.GradientDescentOptimizer(learn_rate)\n",
        "  train_op3 = optimizer3.minimize(loss3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EfKWL5IZigJg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#implementation of the evaluation procedure \n",
        "with tf.name_scope(\"eval3\"):\n",
        "  correct3 = tf.nn.in_top_k(logits,y,1)\n",
        "  accuracy3 = tf.reduce_mean(tf.cast(correct3, tf.float32))\n",
        "  accuracy_summary3 = tf.summary.scalar('accuracy3', accuracy3)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SKcVSGXOAGGT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- - -\n",
        "## 3. EXECUTION PHASE (30 pts)\n",
        "\n",
        "After you construct the three models of neural networks, you can compute the performance measure as the class accuracy. You will need to define the number of epochs and size of the training batch. You also might need to reset the graph each time your try a different model. To save time and avoid retraining, you should save the trained model and load it from disk to evaluate a test set. Pick the best model and answer the following:\n",
        "- Which model yields the best performance measure for your dataset? Provide a reason why it yields the best performance.\n",
        "- Why did you pick this many hidden layers?\n",
        "- Provide some justifiable reasons for selecting the number of neurons per hidden layers. \n",
        "- Which activation functions did you use?\n",
        "\n",
        "In the next session you will get a chance to finetune it further .\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "NGDKdeZzAGGV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Your code goes here\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "n_epochs = 20\n",
        "batch_size = 100\n",
        "\n",
        "# shuffle_batch() shuffle the examples in a batch before training\n",
        "def shuffle_batch(X, y, batch_size):\n",
        "    rnd_idx = np.random.permutation(len(X))\n",
        "    n_batches = len(X) // batch_size\n",
        "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
        "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
        "        yield X_batch, y_batch\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MKMqf1gijPwW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1717
        },
        "outputId": "c50280a9-d2c2-4518-9592-8473e3c27782"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  for epoch in range(n_epochs):\n",
        "    \n",
        "    # implementation of the training ops \n",
        "    for X_batch, y_batch in shuffle_batch(x_train, y_train, batch_size):\n",
        "      sess.run(train_op1, feed_dict={X: X_batch, y: y_batch})\n",
        "      sess.run(train_op2, feed_dict={X: X_batch, y: y_batch})\n",
        "      sess.run(train_op3, feed_dict={X: X_batch, y: y_batch})\n",
        "    \n",
        "    a_batch1 = accuracy1.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "    a_batch2 = accuracy2.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "    a_batch3 = accuracy3.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "\n",
        "    # implementation of the validation accuracy \n",
        "    a_valid1 = accuracy1.eval(feed_dict={X: x_valid, y: y_valid})\n",
        "    a_valid2 = accuracy2.eval(feed_dict={X: x_valid, y: y_valid})\n",
        "    a_valid3 = accuracy3.eval(feed_dict={X: x_valid, y: y_valid})\n",
        "\n",
        "    print(epoch, \"Batch accuracy (1): \", a_batch1, \"\\tValidation accuracy (1): \", a_valid1)\n",
        "    print(epoch, \"Batch accuracy (2): \", a_batch2, \"\\tValidation accuracy (2): \", a_valid2)\n",
        "    print(epoch, \"Batch accuracy (3): \", a_batch3, \"\\tValidation accuracy (3): \", a_valid3)\n",
        "    print(\"\\n\")\n",
        "    \n",
        "  save_path = saver.save(sess, \"./my_dnn_model.ckpt\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Batch accuracy (1):  0.72 \tValidation accuracy (1):  0.7589\n",
            "0 Batch accuracy (2):  0.71 \tValidation accuracy (2):  0.7413\n",
            "0 Batch accuracy (3):  0.71 \tValidation accuracy (3):  0.759\n",
            "\n",
            "\n",
            "1 Batch accuracy (1):  0.84 \tValidation accuracy (1):  0.7922\n",
            "1 Batch accuracy (2):  0.8 \tValidation accuracy (2):  0.7867\n",
            "1 Batch accuracy (3):  0.75 \tValidation accuracy (3):  0.7975\n",
            "\n",
            "\n",
            "2 Batch accuracy (1):  0.81 \tValidation accuracy (1):  0.8124\n",
            "2 Batch accuracy (2):  0.84 \tValidation accuracy (2):  0.8056\n",
            "2 Batch accuracy (3):  0.81 \tValidation accuracy (3):  0.8126\n",
            "\n",
            "\n",
            "3 Batch accuracy (1):  0.78 \tValidation accuracy (1):  0.8192\n",
            "3 Batch accuracy (2):  0.81 \tValidation accuracy (2):  0.8163\n",
            "3 Batch accuracy (3):  0.78 \tValidation accuracy (3):  0.8177\n",
            "\n",
            "\n",
            "4 Batch accuracy (1):  0.85 \tValidation accuracy (1):  0.8255\n",
            "4 Batch accuracy (2):  0.86 \tValidation accuracy (2):  0.8223\n",
            "4 Batch accuracy (3):  0.87 \tValidation accuracy (3):  0.8277\n",
            "\n",
            "\n",
            "5 Batch accuracy (1):  0.88 \tValidation accuracy (1):  0.8309\n",
            "5 Batch accuracy (2):  0.88 \tValidation accuracy (2):  0.8275\n",
            "5 Batch accuracy (3):  0.88 \tValidation accuracy (3):  0.8316\n",
            "\n",
            "\n",
            "6 Batch accuracy (1):  0.72 \tValidation accuracy (1):  0.835\n",
            "6 Batch accuracy (2):  0.7 \tValidation accuracy (2):  0.8319\n",
            "6 Batch accuracy (3):  0.67 \tValidation accuracy (3):  0.8369\n",
            "\n",
            "\n",
            "7 Batch accuracy (1):  0.85 \tValidation accuracy (1):  0.8377\n",
            "7 Batch accuracy (2):  0.86 \tValidation accuracy (2):  0.8348\n",
            "7 Batch accuracy (3):  0.83 \tValidation accuracy (3):  0.8387\n",
            "\n",
            "\n",
            "8 Batch accuracy (1):  0.9 \tValidation accuracy (1):  0.8393\n",
            "8 Batch accuracy (2):  0.91 \tValidation accuracy (2):  0.8385\n",
            "8 Batch accuracy (3):  0.87 \tValidation accuracy (3):  0.8415\n",
            "\n",
            "\n",
            "9 Batch accuracy (1):  0.9 \tValidation accuracy (1):  0.8411\n",
            "9 Batch accuracy (2):  0.88 \tValidation accuracy (2):  0.8395\n",
            "9 Batch accuracy (3):  0.88 \tValidation accuracy (3):  0.8432\n",
            "\n",
            "\n",
            "10 Batch accuracy (1):  0.84 \tValidation accuracy (1):  0.8431\n",
            "10 Batch accuracy (2):  0.82 \tValidation accuracy (2):  0.8419\n",
            "10 Batch accuracy (3):  0.83 \tValidation accuracy (3):  0.8445\n",
            "\n",
            "\n",
            "11 Batch accuracy (1):  0.85 \tValidation accuracy (1):  0.8449\n",
            "11 Batch accuracy (2):  0.85 \tValidation accuracy (2):  0.843\n",
            "11 Batch accuracy (3):  0.85 \tValidation accuracy (3):  0.8471\n",
            "\n",
            "\n",
            "12 Batch accuracy (1):  0.79 \tValidation accuracy (1):  0.8465\n",
            "12 Batch accuracy (2):  0.79 \tValidation accuracy (2):  0.8439\n",
            "12 Batch accuracy (3):  0.77 \tValidation accuracy (3):  0.8509\n",
            "\n",
            "\n",
            "13 Batch accuracy (1):  0.82 \tValidation accuracy (1):  0.8504\n",
            "13 Batch accuracy (2):  0.86 \tValidation accuracy (2):  0.8459\n",
            "13 Batch accuracy (3):  0.87 \tValidation accuracy (3):  0.8519\n",
            "\n",
            "\n",
            "14 Batch accuracy (1):  0.87 \tValidation accuracy (1):  0.8463\n",
            "14 Batch accuracy (2):  0.85 \tValidation accuracy (2):  0.8441\n",
            "14 Batch accuracy (3):  0.85 \tValidation accuracy (3):  0.8528\n",
            "\n",
            "\n",
            "15 Batch accuracy (1):  0.87 \tValidation accuracy (1):  0.8543\n",
            "15 Batch accuracy (2):  0.87 \tValidation accuracy (2):  0.8492\n",
            "15 Batch accuracy (3):  0.86 \tValidation accuracy (3):  0.8542\n",
            "\n",
            "\n",
            "16 Batch accuracy (1):  0.84 \tValidation accuracy (1):  0.8538\n",
            "16 Batch accuracy (2):  0.85 \tValidation accuracy (2):  0.8524\n",
            "16 Batch accuracy (3):  0.85 \tValidation accuracy (3):  0.8551\n",
            "\n",
            "\n",
            "17 Batch accuracy (1):  0.8 \tValidation accuracy (1):  0.8566\n",
            "17 Batch accuracy (2):  0.83 \tValidation accuracy (2):  0.8527\n",
            "17 Batch accuracy (3):  0.81 \tValidation accuracy (3):  0.8553\n",
            "\n",
            "\n",
            "18 Batch accuracy (1):  0.86 \tValidation accuracy (1):  0.858\n",
            "18 Batch accuracy (2):  0.85 \tValidation accuracy (2):  0.8535\n",
            "18 Batch accuracy (3):  0.83 \tValidation accuracy (3):  0.8572\n",
            "\n",
            "\n",
            "19 Batch accuracy (1):  0.89 \tValidation accuracy (1):  0.8583\n",
            "19 Batch accuracy (2):  0.88 \tValidation accuracy (2):  0.8535\n",
            "19 Batch accuracy (3):  0.88 \tValidation accuracy (3):  0.8561\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XKr30RKa_do9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "94956026-d505-41f2-c900-2f45127bb186"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, save_path)\n",
        "  accuracy_test1 = accuracy1.eval(feed_dict={X: x_test, y: y_test})\n",
        "  accuracy_test2 = accuracy2.eval(feed_dict={X: x_test, y: y_test})\n",
        "  accuracy_test3 = accuracy3.eval(feed_dict={X: x_test, y: y_test})\n",
        "\n",
        "print('Accuracy of dnn1: ', accuracy_test1)\n",
        "print('Accuracy of dnn2: ', accuracy_test2)\n",
        "print('Accuracy of dnn3: ', accuracy_test3)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of dnn1:  0.8443\n",
            "Accuracy of dnn2:  0.8431\n",
            "Accuracy of dnn3:  0.8444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "syt9OgXx0qwA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "EXPLANATION:\n",
        "\n",
        "My first model had the best performance measures. This model had three hidden layers, two of which use an relu activation function, while the middle layer uses a tanh activation function. I tested models with 2, 3, and 5 hidden layers, over several different activation function combination, and this was the most successful model.  I adjusted the number of neurons up and down a bit, to ensure that the final number of neurons per hidden layer were ideal. The testing accuracy of this model was .8444.\n"
      ]
    },
    {
      "metadata": {
        "id": "-s2zv1SrAGGY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- - -\n",
        "## 4. FINETUNING THE NETWORK (25 pts)\n",
        "\n",
        "The best performance on the Fashion MNIST of a non-neural-net classifier is the Support Vector Classifier {\"C\":10,\"kernel\":\"poly\"} with 0.897 accuracy. In this section, you will see how close you can get to that accuracy, or (better yet) beat it! You will be able to see the performance of other ML methods below:\n",
        "http://fashion-mnist.s3-website.eu-central-1.amazonaws.com\n",
        "\n",
        "Use the best model from the previous section and see if you can improve it further. To improve the performance of your model, You must make some modifications based upon the practical guidelines discuss in class. Here are a few decisions about the recommended network configurations you have to make:\n",
        "1. Initialization: Use He Initialization for your model\n",
        "2. Activation: Add ELU as the activation function throughout your hidden layers\n",
        "3. Normalization: Incorporate the batch normalization at every layer\n",
        "4. Regularization: Configure the dropout policy at 50% rate\n",
        "5. Optimization: Change Gradient Descent into Adam Optimization\n",
        "6. Your choice: make any other changes in 1-5 you deem necessary\n",
        "\n",
        "Keep in mind that the execution phase is essentially the same, so you can just run it from the above. See how much you gain in classification accuracy. Provide some justifications for the gain in performance. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "hjDXZ5ws6Zpx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "n_inputs = 28 * 28\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 150\n",
        "n_hidden3 = 100\n",
        "n_outputs = 10\n",
        "\n",
        "#4. Regularization: Configure the dropout policy at 50% rate\n",
        "dropout_rate = .5\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
        "\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "# implementation of the new benchmarking DNN\n",
        "with tf.name_scope(\"dnnBenchmark\"):\n",
        "  #1. Initialization: Use He Initialization for your model\n",
        "  he_init = tf.variance_scaling_initializer()\n",
        "  \n",
        "  #3. Normalization: Incorporate the batch normalization at every layer\n",
        "  my_batch_norm_layer = partial(\n",
        "            tf.layers.batch_normalization,\n",
        "            training=training,\n",
        "            momentum=.9)\n",
        "  \n",
        "  hidden1BM = tf.layers.dense(X, n_hidden1, name=\"hidden1BM\", activation=tf.nn.relu, kernel_initializer=he_init)\n",
        "  hidden1_drop = tf.layers.dropout(hidden1BM, dropout_rate, training=training)\n",
        "  bn1 = tf.nn.elu(my_batch_norm_layer(hidden1_drop))\n",
        "  \n",
        "  hidden2BM = tf.layers.dense(X, n_hidden2, name=\"hidden2BM\", activation=tf.nn.relu, kernel_initializer=he_init)\n",
        "  hidden2_drop = tf.layers.dropout(hidden2BM, dropout_rate, training=training)\n",
        "  bn2 = tf.nn.elu(my_batch_norm_layer(hidden2_drop))\n",
        "  \n",
        "  hidden3BM = tf.layers.dense(X, n_hidden3, name=\"hidden3BM\", activation=tf.nn.relu, kernel_initializer=he_init)\n",
        "  hidden3_drop = tf.layers.dropout(hidden3BM, dropout_rate, training=training)\n",
        "  bn3 = tf.nn.elu(my_batch_norm_layer(hidden3_drop))\n",
        "  \n",
        "  logits_before_bn = tf.layers.dense(bn3, n_outputs, name = \"outputsBM\", kernel_initializer=he_init)\n",
        "  logits = my_batch_norm_layer(logits_before_bn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oUFR-o0toVdW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#implementation of the loss function net\n",
        "with tf.name_scope(\"lossBM\"):\n",
        "  xentropyBM = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "  lossBM = tf.reduce_mean(xentropyBM, name=\"lossBM\")\n",
        "  loss_summaryBM = tf.summary.scalar('log_lossBM', lossBM)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VUfglJJmoYm-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#5 use Adam Optimization\n",
        "\n",
        "learn_rate = .01\n",
        "\n",
        "#implementation of the training optimizer\n",
        "with tf.name_scope(\"trainBM\"):\n",
        "  optimizerBM = tf.train.AdamOptimizer(learn_rate)\n",
        "  train_opBM = optimizerBM.minimize(lossBM)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "URS5kIIYoc-k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#implementation of the evaluation procedure here\n",
        "with tf.name_scope(\"evalBM\"):\n",
        "  correctBM = tf.nn.in_top_k(logits,y,1)\n",
        "  accuracyBM = tf.reduce_mean(tf.cast(correctBM, tf.float32))\n",
        "  accuracy_summaryBM = tf.summary.scalar('accuracyBM', accuracyBM)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zWfAbhEZx3ul",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "n_epochs = 20\n",
        "batch_size = 50\n",
        "\n",
        "# shuffle_batch() shuffle the examples in a batch before training\n",
        "def shuffle_batch(X, y, batch_size):\n",
        "    rnd_idx = np.random.permutation(len(X))\n",
        "    n_batches = len(X) // batch_size\n",
        "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
        "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
        "        yield X_batch, y_batch\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NovVYCQTx278",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "25738f72-2c68-42a5-fcd5-d2153eb0a0ed"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  for epoch in range(n_epochs):\n",
        "    for X_batch, y_batch in shuffle_batch(x_train, y_train, batch_size):\n",
        "      sess.run(train_opBM, feed_dict={X: X_batch, y: y_batch})\n",
        "    \n",
        "    a_batchBM = accuracyBM.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "    a_validBM = accuracyBM.eval(feed_dict={X: x_valid, y: y_valid})\n",
        "\n",
        "    print(epoch, \"Batch accuracy: \", a_batchBM, \"\\tValidation accuracy: \", a_validBM)\n",
        "    \n",
        "  save_path = saver.save(sess, \"./my_dnn_model.ckpt\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Batch accuracy:  0.9 \tValidation accuracy:  0.8525\n",
            "1 Batch accuracy:  0.82 \tValidation accuracy:  0.8485\n",
            "2 Batch accuracy:  0.9 \tValidation accuracy:  0.8333\n",
            "3 Batch accuracy:  0.82 \tValidation accuracy:  0.8734\n",
            "4 Batch accuracy:  0.86 \tValidation accuracy:  0.8758\n",
            "5 Batch accuracy:  0.96 \tValidation accuracy:  0.863\n",
            "6 Batch accuracy:  0.8 \tValidation accuracy:  0.879\n",
            "7 Batch accuracy:  0.92 \tValidation accuracy:  0.8795\n",
            "8 Batch accuracy:  0.9 \tValidation accuracy:  0.8768\n",
            "9 Batch accuracy:  0.9 \tValidation accuracy:  0.8697\n",
            "10 Batch accuracy:  0.82 \tValidation accuracy:  0.875\n",
            "11 Batch accuracy:  0.86 \tValidation accuracy:  0.8818\n",
            "12 Batch accuracy:  0.8 \tValidation accuracy:  0.8702\n",
            "13 Batch accuracy:  0.9 \tValidation accuracy:  0.8814\n",
            "14 Batch accuracy:  0.98 \tValidation accuracy:  0.8705\n",
            "15 Batch accuracy:  0.92 \tValidation accuracy:  0.8782\n",
            "16 Batch accuracy:  0.86 \tValidation accuracy:  0.8858\n",
            "17 Batch accuracy:  0.86 \tValidation accuracy:  0.8816\n",
            "18 Batch accuracy:  0.92 \tValidation accuracy:  0.8767\n",
            "19 Batch accuracy:  0.9 \tValidation accuracy:  0.8776\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yoP2Djqj-x_J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed189bc2-074b-401d-959d-6d6cfb153fee"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, save_path)\n",
        "  accuracy_testBM = accuracyBM.eval(feed_dict={X: x_test, y: y_test})\n",
        "\n",
        "print('Accuracy of dnnbenchmark: ', accuracy_testBM)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of dnnbenchmark:  0.8729\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pvRnd2_l00MQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For this part, I selected my best performing model, the one with three hidden layers, to fine tune further. By making the above improvements, I was able to successfully increase my testing accuracy. I used He initialization, seleccted ELU as the activation function for all hidden layers, implemented batch normalization for each layer, implemented a 50% dropout rate, and changed the optimization function to Adam Optimization. By making the above changes, I improved my testing accuracy from .844 to .8729. This improvement can be attributed to a number of things, most importantly the implementation of a dropout rate. This dropout rates can prevent overfitting of data, so its inclusion here is essential ot the improvement of results. \n"
      ]
    },
    {
      "metadata": {
        "id": "04jsbI9TAGGY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- - -\n",
        "## 5. OUTLOOK (5 pts)\n",
        "\n",
        "Plan for the outlook of your system: This may lead to the direction of your future project:\n",
        "- Did your neural network outperform other \"traditional ML technique? Why/why not?\n",
        "- Does your model work well? If not, which model should be further investigated?\n",
        "- Do you satisfy with your system? What do you think needed to improve?\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "vaEO5BBgIi3R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I think my model works pretty well. There are improvements that can still be made, such as further tuning of the number of neurons per layer, but overall my model preforms well. Compared to the accuracy of other models found at http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/ my model performs very well, having an .8729 accuracy score. This is significantly better than almost all of the other types of models found in the chart. There are only a few types of model that seems to perform above a .87 accuracy, including a Random Forest Classifier, Gradient Boosting Classifier, and Support Vector Classifier, as mentioned in the introduction to section 4.\n",
        "\n",
        "Although there is always room for improvement, I am satisfied with my model, and feel a .8729 accuracy is pretty good.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "zS9PKaL4AGGZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- - - \n",
        "### NEED HELP?"
      ]
    },
    {
      "metadata": {
        "id": "T0vuIEBDAGGa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In case you get stuck in any step in the process, you may find some useful information from:\n",
        "\n",
        " * Consult my lectures and/or the textbook\n",
        " * Talk to the TA, they are available and there to help you during OH\n",
        " * Come talk to me or email me <nn4pj@virginia.edu> with subject starting \"CS4501 Assignment 4:...\".\n",
        " * More on the Fashion-MNIST to be found here: https://hanxiao.github.io/2018/09/28/Fashion-MNIST-Year-In-Review/\n",
        "\n",
        "Best of luck and have fun!"
      ]
    },
    {
      "metadata": {
        "id": "cH_mulWEAGGb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}